#!/bin/bash
#SBATCH --job-name=fddl_lor
#SBATCH --output=logs/%j.out
#SBATCH --error=logs/%j.err
#SBATCH --nodes=4
#SBATCH --ntasks=16
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=1
#SBATCH --gpus-per-node=4
#SBATCH --gres=gpu:4
#SBATCH --time=00:05:00
#SBATCH --partition=boost_usr_prod
#SBATCH --account=IscrB_SWING
#--qos=boost_qos_bprod
#--exclusive

#module purge
#module load anaconda3
#module load cuda/12.2

#conda env create -f my_env.yml
#conda activate fddl_env

export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=12355
export WORLD_SIZE=$SLURM_NTASKS

#srun python autoencoder_ddp/train.py --ntasks $SLURM_NTASKS

srun torchrun --nnodes=$SLURM_NNODES --nproc_per_node=4 --rdzv_id=$SLURM_JOB_ID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT train.py --ntasks $SLURM_NTASKS