W0701 12:25:51.488000 3079135 site-packages/torch/distributed/run.py:766] 
W0701 12:25:51.488000 3079135 site-packages/torch/distributed/run.py:766] *****************************************
W0701 12:25:51.488000 3079135 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0701 12:25:51.488000 3079135 site-packages/torch/distributed/run.py:766] *****************************************
W0701 12:25:51.488000 3079136 site-packages/torch/distributed/run.py:766] 
W0701 12:25:51.488000 3079136 site-packages/torch/distributed/run.py:766] *****************************************
W0701 12:25:51.488000 3079136 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0701 12:25:51.488000 3079136 site-packages/torch/distributed/run.py:766] *****************************************
srun: Job step aborted: Waiting up to 92 seconds for job step to finish.
slurmstepd: error: *** STEP 17194063.1 ON lrdn0129 CANCELLED AT 2025-07-01T12:26:25 ***
slurmstepd: error: *** JOB 17194063 ON lrdn0129 CANCELLED AT 2025-07-01T12:26:25 ***
W0701 12:26:25.433000 3079136 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0701 12:26:25.434000 3079136 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3079167 closing signal SIGTERM
W0701 12:26:25.435000 3079136 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3079168 closing signal SIGTERM
